# NLP_intent_classifier_SHEVCHUK_TAVERNIER
Machine Learning for NLP – ENSAE 2023

Owing to the tremendous improvements made
in Deep Learning over the past few years,
emotion recognition has become a topic of
growing interest. In this respect, we have wit-
nessed a huge development of dialogue gener-
ation and conversational systems, with Chat-
GPT as leading figure. Indeed, owing to its
wide spectrum of potential applications, such
as dialogue generation and conversational
systems, emotion recognition is key to generate
an appropriate answer and avoid the ”generic
response problem”, with a view to providing
the best possible user experience. By virtue
of the widespread access to plug-and-play
pre-trained NLP models (e.g., BERT, GPT-
4, etc.), implementing conversational systems
has never seemed so easy and straightforward.
However, under closer scrutiny, deploying an
efficient end-to-end conversational system en-
compasses a fair amount of challenges (e.g.,
fillers, code-switching, communicative intent
identification, etc.) that researchers still en-
deavor to mitigate. In fact, spoken language
and oral interactions are usually scrappy, en-
compassing fillers and less formal, notably
from both grammatical and syntactical per-
spectives.
In this paper, we precisely aim at assess-
ing whether current state-of-the-art pretained
NLP models that have hit the headlines over
the past few years for their unprecedented ca-
pabilities, can really cope with spoken lan-
guage when it comes to speech emotion recog-
nition. More specifically, we deploy BERT on
a dataset of transcribed oral conversations, to
evaluate its performances.
