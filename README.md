# NLP_intent_classifier_SHEVCHUK_TAVERNIER
Machine Learning for NLP â€“ ENSAE 2023

Owing to the tremendous improvements made in Deep Learning over the past few years, emotion recognition has become a topic of growing interest. In this respect, we have witnessed a huge development of dialogue generation and conversational systems, with ChatGPT as leading figure. Indeed, owing to its wide spectrum of potential applications, such as dialogue generation and conversational systems, emotion recognition is key to generate an appropriate answer and avoid the "generic response problem", with a view to providing the best possible user experience. By virtue of the widespread access to plug-and-play pre-trained NLP models (e.g., BERT, GPT-4, etc.), implementing conversational systems has never seemed so easy and straightforward. However, under closer scrutiny, deploying an efficient end-to-end conversational system encompasses a fair amount of challenges (e.g., fillers, code-switching, communicative intent identification, etc.) that researchers still endeavor to mitigate. As a matter of fact, spoken language and oral interactions are usually scrappy as well as less formal, notably from both grammatical and syntactical perspectives.  

Throughout this paper, we precisely aim at assessing whether current state-of-the-art pre-trained NLP models, that have recently been hitting the headlines due to their unprecedented capabilities, can efficiently cope with spoken language. More specifically, we deploy BERT on a dataset of transcribed oral conversations, to evaluate its performances.
