# NLP_intent_classifier_SHEVCHUK_TAVERNIER
Machine Learning for NLP – ENSAE 2023

Owing to the tremendous improvements made
in Deep Learning over the past few years,
emotion recognition has become a topic of
growing interest. In this respect, we have witnessed a huge development of dialogue 
generation and conversational systems, with ChatGPT as leading figure. Indeed, owing to its
wide spectrum of potential applications, such
as dialogue generation and conversational
systems, emotion recognition is key to generate
an appropriate answer and avoid the ”generic
response problem”, with a view to providing
the best possible user experience. By virtue
of the widespread access to plug-and-play
pre-trained NLP models (e.g., BERT, GPT-
4, etc.), implementing conversational systems
has never seemed so easy and straightforward.
However, under closer scrutiny, deploying an
efficient end-to-end conversational system encompasses a fair amount of challenges (e.g.,
fillers, code-switching, communicative intent
identification, etc.) that researchers still endeavor to mitigate. In fact, spoken language
and oral interactions are usually scrappy, encompassing fillers and less formal, notably
from both grammatical and syntactical perspectives.
In this paper, we precisely aim at assessing whether current state-of-the-art pretained
NLP models that have hit the headlines over
the past few years for their unprecedented capabilities, can really cope with spoken language when it comes to speech emotion recognition. More specifically, we deploy BERT on
a dataset of transcribed oral conversations, to
evaluate its performances.
